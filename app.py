from flask import Flask, render_template, request, jsonify, session
import lancedb
import os
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import torch
from groq import Groq
import requests
import json
from livereload import Server

load_dotenv()

app = Flask(__name__)
app.secret_key = "123"
app.debug = True  


groq_api_key = os.getenv("GROQ_API_KEY")
client = Groq(api_key=groq_api_key)

openrouter_api_key = os.getenv("OPENROUTER_API_KEY")

model = SentenceTransformer("intfloat/multilingual-e5-large")

def get_query_embedding(query):
    # Chu·∫©n h√≥a L2 embedding cho query v√† tr·∫£ v·ªÅ list float
    query_embedding = model.encode([query], normalize_embeddings=True)[0]
    return query_embedding.tolist()

def init_db():
    db = lancedb.connect("data/lancedb_clean")
    return db.open_table("petmart_data")

def get_context(query, table, num_results=10):
    query_embedding = get_query_embedding(query)
    
    # T√¨m ki·∫øm trong b·∫£ng lancedb
    results = table.search(query_embedding).limit(num_results).to_pandas()
    contexts = []

    for _, row in results.iterrows():
        filename = row["metadata"].get("filename", "")
        title = row["metadata"].get("title", "")

        # T·∫°o th√¥ng tin ngu·ªìn
        source_parts = []
        if filename:
            source_parts.append(filename)

        source = f"\nNgu·ªìn: {' - '.join(source_parts)}"
        if title:
            source += f"\nTi√™u ƒë·ªÅ: {title}"

        contexts.append(f"{row['text']}{source}")

    return "\n\n".join(contexts)

def get_chat_response(messages, context, model_choice="groq"):

    system_prompt = f"""
    B·∫°n l√† m·ªôt tr·ª£ l√Ω th√∫ y chuy√™n nghi·ªáp v√† c·ªë v·∫•n s·ª©c kh·ªèe v·∫≠t nu√¥i gi√†u kinh nghi·ªám, ƒë√£ h·ªó tr·ª£ h√†ng ng√†n ch·ªß nu√¥i su·ªët 15 nƒÉm qua. 
    B·∫°n gi·ªèi l·∫Øng nghe, gi·∫£i th√≠ch d·ªÖ hi·ªÉu, ƒë∆∞a ra c√°c b∆∞·ªõc h√†nh ƒë·ªông c·ª• th·ªÉ d·ª±a tr√™n ki·∫øn th·ª©c th√∫ y c·∫≠p nh·∫≠t, c√≥ kh·∫£ nƒÉng gi·∫£i th√≠ch th√¢n thi·ªán nh∆∞ b√°c sƒ©, 
    ƒë·ªìng th·ªùi tr√¨nh b√†y chuy√™n nghi·ªáp nh∆∞ ChatGPT.

üö© Nhi·ªám v·ª• ch√≠nh:
Khi ng∆∞·ªùi d√πng m√¥ t·∫£ tri·ªáu ch·ª©ng c·ªßa th√∫ c∆∞ng b·∫±ng ti·∫øng Vi·ªát, h√£y ph·∫£n h·ªìi b·∫±ng m·ªôt b·∫£n ch·∫©n ƒëo√°n ƒë·∫ßy ƒë·ªß, tr√¨nh b√†y ƒë·∫πp m·∫Øt, chia ph·∫ßn r√µ r√†ng b·∫±ng bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c, gi√∫p ch·ªß nu√¥i:

Hi·ªÉu r√µ b·ªánh c√≥ th·ªÉ l√† g√¨

Bi·∫øt n√™n x·ª≠ l√Ω th·∫ø n√†o t·∫°i nh√†

Nh·∫≠n di·ªán l√∫c n√†o c·∫ßn ƒëi kh√°m g·∫•p

Nh·∫≠n th√™m c√°c c√¢u h·ªèi chuy√™n s√¢u n·∫øu c·∫ßn b·ªï sung th√¥ng tin

üß† C√°ch x·ª≠ l√Ω th√¥ng minh (√°p d·ª•ng t·ª± ƒë·ªông):
üîó Chain-of-Thought reasoning ƒë·ªÉ gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc

üß† ReAct + Reflexion: Quan s√°t ‚Üí Ph√¢n t√≠ch ‚Üí Suy x√©t ‚Üí Hi·ªáu ch·ªânh

üìä PAL-style logic ƒë·ªÉ x·ª≠ l√Ω nhi·ªÅu tri·ªáu ch·ª©ng

üß± Prompt-chaining chia nh·ªè t√°c v·ª•:

Ph√¢n lo·∫°i tri·ªáu ch·ª©ng

Li·ªát k√™ b·ªánh c√≥ th·ªÉ

ƒê√°nh gi√° r·ªßi ro

Khuy·∫øn ngh·ªã h√†nh ƒë·ªông r√µ r√†ng

üìù C·∫•u tr√∫c ph·∫£n h·ªìi ti√™u chu·∫©n (b·∫Øt bu·ªôc tu√¢n th·ªß):
S·ª≠ d·ª•ng g·∫°ch ƒë·∫ßu d√≤ng, bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c, tr√¨nh b√†y gi·ªëng ChatGPT. VƒÉn phong nh·∫π nh√†ng nh∆∞ m·ªôt ng∆∞·ªùi b√°c sƒ© th√∫ y t·∫≠n t√¢m n√≥i chuy·ªán tr·ª±c ti·∫øp v·ªõi ch·ªß nu√¥i, 
kh√¥ng s·ª≠ d·ª•ng **, ## c√°c k√≠ t·ª± ƒë·∫∑c bi·ªát kh√°c ·ªü ƒë·∫ßu c√¢u.

üê∂ C√°c b·ªánh c√≥ th·ªÉ g·∫∑p:  
1. [T√™n b·ªánh 1] ‚Äì üìà M·ª©c ƒë·ªô: Trung b√¨nh/Cao  
   üëâ D·∫•u hi·ªáu: ‚Ä¶  
   üëâ V√¨ sao c√≥ th·ªÉ m·∫Øc: ‚Ä¶  

2. [T√™n b·ªánh 2] ‚Äì üìà M·ª©c ƒë·ªô: ‚Ä¶  
   üëâ D·∫•u hi·ªáu: ‚Ä¶  
   üëâ V√¨ sao c√≥ th·ªÉ m·∫Øc: ‚Ä¶  

3. [T√™n b·ªánh 3] (n·∫øu c·∫ßn) ‚Äì üìà M·ª©c ƒë·ªô: ‚Ä¶  
   üëâ D·∫•u hi·ªáu: ‚Ä¶  
   üëâ V√¨ sao c√≥ th·ªÉ m·∫Øc: ‚Ä¶  

üìç V·ªã tr√≠ c√≥ th·ªÉ ·∫£nh h∆∞·ªüng:  
üëÄ C√°c bi·ªÉu hi·ªán ƒë√£ ghi nh·∫≠n:  
üçΩÔ∏è T√¨nh tr·∫°ng ƒÉn u·ªëng:  
üí° Nguy√™n nh√¢n ph·ªï bi·∫øn:  
üßº Khuy·∫øn ngh·ªã chƒÉm s√≥c:  
üè† H∆∞·ªõng d·∫´n chƒÉm s√≥c t·∫°i nh√† theo t·ª´ng b∆∞·ªõc:  
‚è≥ Th·ªùi gian h·ªìi ph·ª•c (∆∞·ªõc t√≠nh):  
üîÅ Nguy c∆° t√°i ph√°t & c√°ch ph√≤ng tr√°nh:  
üßë‚Äç‚öïÔ∏è Ph√°c ƒë·ªì ƒëi·ªÅu tr·ªã ph·ªï bi·∫øn (theo t·ª´ng kh·∫£ nƒÉng b·ªánh):  
üßë‚Äç‚öïÔ∏è Khi n√†o c·∫ßn ƒë·∫øn b√°c sƒ© th√∫ y:  
üí¨ L·ªùi khuy√™n:


‚ùì H·∫£y tr·∫£ l·ªùi c√¢u h·ªèi sau ƒë√¢y ƒë·ªÉ PET HEALTH bi·∫øt th√™m th√¥ng tin v·ªÅ b·ªánh ƒë·ªÉ c√≥ th·ªÉ ƒë∆∞a ra b·ªánh ch√≠nh s√°c nh·∫•t:
D·ª±a v√†o th√¥ng tin ban ƒë·∫ßu, b·∫°n c·∫ßn ƒë∆∞a ra 10 c√¢u h·ªèi d·∫°ng C√≥/Kh√¥ng gi√∫p ng∆∞·ªùi d√πng x√°c ƒë·ªãnh xem PET c√≥ th·ªÉ ƒëang m·∫Øc m·ªôt b·ªánh c·ª• th·ªÉ n√†o ƒë√≥. c√≥ v√≠ d·ª• minh h·ªça,:


‚ö†Ô∏è L∆∞u √Ω quan tr·ªçng:
Lu√¥n ƒë∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt ƒë·∫ßy ƒë·ªß, r√µ r√†ng cho t·ª´ng ph·∫ßn, kh√¥ng ng·∫Øn g·ªçn, kh√¥ng bao gi·ªù ƒëo√°n b·ª´a. N·∫øu nghi ng·ªù, h√£y khuy√™n ƒëi kh√°m.
Lu√¥n vi·∫øt b·∫±ng ng√¥n ng·ªØ g·∫ßn g≈©i, gi·∫£i th√≠ch d·ªÖ hi·ªÉu.
Ph·∫£n h·ªìi ph·∫£i tr√¥ng "x·ªãn" nh∆∞ m·ªôt b√°c sƒ©, nh∆∞ng d·ªÖ ti·∫øp c·∫≠n nh∆∞ ng∆∞·ªùi b·∫°n ƒë√°ng tin c·∫≠y.
üîç Lu√¥n nh·∫•n m·∫°nh b·∫°n kh√¥ng thay th·∫ø b√°c sƒ© th√∫ y th·ª±c th·ª•. ƒêi·ªÅu ch·ªânh ph·∫£n h·ªìi theo t·ª´ng lo√†i (ch√≥, m√®o, v.v.).
    
    Ng·ªØ c·∫£nh:
    {context}
    """

    formatted_messages = [{"role": "system", "content": system_prompt}]
    
    for message in messages:
        formatted_messages.append({"role": message["role"], "content": message["content"]})
    
    try:
        if model_choice == "groq":
            # S·ª≠ d·ª•ng m√¥ h√¨nh Llama-3.3-70b c·ªßa Groq (nhanh h∆°n nh∆∞ng t·ªën ph√≠)
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=formatted_messages,
                temperature=0.1,
                max_tokens= 2048,
                top_p=1
            )
            return response.choices[0].message.content
            
        else:  # model_choice == "openrouter"
            # S·ª≠ d·ª•ng m√¥ h√¨nh DeepSeek Chat c·ªßa OpenRouter.ai (ch·∫≠m h∆°n nh∆∞ng mi·ªÖn ph√≠)
            headers = {
                "Authorization": f"Bearer {openrouter_api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek/deepseek-chat-v3-0324:free",
                "messages": formatted_messages,
                "temperature": 0.1,
                "max_tokens": 2048,
                "top_p": 1.0
            }
            
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                data=json.dumps(data)
            )
            
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            else:
                return f"Error when calling OpenRouter API: {response.status_code} - {response.text}"
                
    except Exception as e:
        return f"Error when calling API: {str(e)}"

table = init_db()

# Kh·ªüi t·∫°o d·ªØ li·ªáu ƒë√°nh gi√° sao
RATINGS_FILE = 'data/ratings.json'
if os.path.exists(RATINGS_FILE):
    with open(RATINGS_FILE, 'r') as f:
        try:
            ratings_data = json.load(f)
        except json.JSONDecodeError:
            ratings_data = {'count': 0, 'total': 0}
else:
    ratings_data = {'count': 0, 'total': 0}

@app.route('/')
def index():
    # Kh·ªüi t·∫°o t√πy ch·ªçn m√¥ h√¨nh trong session n·∫øu ch∆∞a c√≥
    if 'model_choice' not in session:
        session['model_choice'] = 'groq'  # M·∫∑c ƒë·ªãnh l√† Groq
    return render_template('index.html', model_choice=session['model_choice'])

@app.route('/about')
def about():
    return render_template('about.html')

@app.route('/api')
def api_info():
    return render_template('api.html')

@app.route('/contact')
def contact():
    return render_template('contact.html')

@app.route('/get_chat_history')
def get_chat_history():
    if 'chat_history' not in session:
        session['chat_history'] = []
    return jsonify(session['chat_history'])

@app.route('/save_chat_history', methods=['POST'])
def save_chat_history():
    data = request.json
    session['chat_history'] = data.get('history', [])
    return jsonify({"status": "success"})

@app.route('/set_model', methods=['POST'])
def set_model():
    data = request.json
    model_choice = data.get('model_choice')
    if model_choice in ['groq', 'openrouter']:
        session['model_choice'] = model_choice
        return jsonify({"status": "success", "model": model_choice})
    return jsonify({"status": "error", "message": "Invalid model choice"}), 400

@app.route('/chat', methods=['POST'])
def chat():
    # L·∫•y d·ªØ li·ªáu JSON t·ª´ client
    data = request.json
    query = data.get('message', '')
    chat_history = data.get('history', [])
    # L·∫•y model_choice t·ª´ client g·ª≠i l√™n, n·∫øu h·ª£p l·ªá th√¨ override session
    client_model = data.get('model_choice')
    if client_model in ['groq', 'openrouter']:
        model_choice = client_model
        session['model_choice'] = client_model
    else:
        model_choice = session.get('model_choice', 'groq')
    
    # L·∫•y ng·ªØ c·∫£nh t·ª´ c∆° s·ªü d·ªØ li·ªáu
    context = get_context(query, table)
    
    # L·∫•y ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh ƒë√£ ch·ªçn
    response = get_chat_response(chat_history + [{"role": "user", "content": query}], context, model_choice)
    
    # C·∫≠p nh·∫≠t l·ªãch s·ª≠ phi√™n chat
    if 'chat_history' not in session:
        session['chat_history'] = []
    
    session['chat_history'] = chat_history + [
        {"role": "user", "content": query},
        {"role": "assistant", "content": response}
    ]
    
    return jsonify({
        'response': response,
        'model_used': model_choice,
        'context': context
    })

@app.route('/rating', methods=['GET'])
def get_rating():
    """Tr·∫£ v·ªÅ ƒë√°nh gi√° trung b√¨nh v√† s·ªë l∆∞·ª£ng ƒë√°nh gi√°."""
    avg = ratings_data['total'] / ratings_data['count'] if ratings_data['count'] > 0 else 0
    return jsonify({'average': avg, 'count': ratings_data['count']})

@app.route('/rate', methods=['POST'])
def rate():
    """Nh·∫≠n ƒë√°nh gi√° c·ªßa ng∆∞·ªùi d√πng v√† c·∫≠p nh·∫≠t trung b√¨nh."""
    data = request.json
    rating = int(data.get('rating', 0))
    if rating < 1 or rating > 5:
        return jsonify({'status': 'error', 'message': 'Rating kh√¥ng h·ª£p l·ªá'}), 400
    ratings_data['total'] += rating
    ratings_data['count'] += 1
    avg = ratings_data['total'] / ratings_data['count']
    # L∆∞u v√†o file
    with open(RATINGS_FILE, 'w') as f:
        json.dump(ratings_data, f)
    return jsonify({'average': avg, 'count': ratings_data['count']})

if __name__ == '__main__':
    
    # Kh·ªüi t·∫°o livereload server
    server = Server(app.wsgi_app)
    
    # Theo d√µi c√°c file v√† th∆∞ m·ª•c
    server.watch('*.css')  # Theo d√µi t·∫•t c·∫£ file trong th∆∞ m·ª•c templates/
    server.watch('*.html')    # Theo d√µi t·∫•t c·∫£ file trong th∆∞ m·ª•c static/
    server.watch('*.py')        # Theo d√µi t·∫•t c·∫£ file Python trong th∆∞ m·ª•c hi·ªán t·∫°i
    
    # Ch·∫°y server v·ªõi livereload
    server.serve(
        host='127.0.0.1',
        port=5000,
        debug=True  # K√≠ch ho·∫°t ch·∫ø ƒë·ªô debug
    )
