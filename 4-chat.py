import streamlit as st
import lancedb
from dotenv import load_dotenv
import os
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from groq import Groq


# Táº£i biáº¿n mÃ´i trÆ°á»ng
load_dotenv()

# Khá»Ÿi táº¡o Groq client
groq_api_key = os.getenv("GROQ_API_KEY")
client = Groq(api_key=groq_api_key)

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh embedding
@st.cache_resource
def load_embedding_model():
    model_name = "intfloat/multilingual-e5-large"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    return tokenizer, model

# HÃ m táº¡o embedding cho cÃ¢u truy váº¥n
def get_query_embedding(query_text, tokenizer, model):
    """
    Táº¡o embedding cho cÃ¢u truy váº¥n sá»­ dá»¥ng mÃ´ hÃ¬nh multilingual-e5-large
    """
    # Chuáº©n bá»‹ input
    inputs = tokenizer([query_text], padding=True, truncation=True, 
                    max_length=512, return_tensors="pt")
    
    # TÃ­nh embedding
    with torch.no_grad():
        outputs = model(**inputs)
        # Láº¥y embedding cá»§a token [CLS]
        embeddings = outputs.last_hidden_state[:, 0]
    
    # Chuáº©n hÃ³a embedding
    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
    
    # Chuyá»ƒn vá» numpy array
    return embeddings[0].cpu().numpy().tolist()

# Khá»Ÿi táº¡o káº¿t ná»‘i LanceDB
@st.cache_resource
def init_db():
    """Khá»Ÿi táº¡o káº¿t ná»‘i Ä‘áº¿n cÆ¡ sá»Ÿ dá»¯ liá»‡u.

    Returns:
        LanceDB table object
    """
    db = lancedb.connect("data/lancedb_clean")  # Sá»­ dá»¥ng thÆ° má»¥c lancedb má»›i
    return db.open_table("petmart_data")


def get_context(query: str, table, tokenizer, model, num_results: int = 10) -> str:
    """TÃ¬m kiáº¿m trong cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ láº¥y ngá»¯ cáº£nh liÃªn quan.

    Args:
        query: CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
        table: Äá»‘i tÆ°á»£ng báº£ng LanceDB
        tokenizer: Tokenizer Ä‘á»ƒ xá»­ lÃ½ vÄƒn báº£n
        model: MÃ´ hÃ¬nh Ä‘á»ƒ táº¡o embedding
        num_results: Sá»‘ káº¿t quáº£ tráº£ vá»

    Returns:
        str: Ngá»¯ cáº£nh káº¿t há»£p tá»« cÃ¡c Ä‘oáº¡n liÃªn quan kÃ¨m thÃ´ng tin nguá»“n
    """
    # Táº¡o embedding cho query
    query_embedding = get_query_embedding(query, tokenizer, model)
    
    # TÃ¬m kiáº¿m trong báº£ng
    results = table.search(query_embedding).limit(num_results).to_pandas()
    contexts = []

    for _, row in results.iterrows():
        # Láº¥y metadata
        filename = row["metadata"].get("filename", "")
        title = row["metadata"].get("title", "")

        # Táº¡o thÃ´ng tin nguá»“n
        source_parts = []
        if filename:
            source_parts.append(filename)

        source = f"\nNguá»“n: {' - '.join(source_parts)}"
        if title:
            source += f"\nTiÃªu Ä‘á»: {title}"

        contexts.append(f"{row['text']}{source}")

    return "\n\n".join(contexts)


def get_chat_response(messages, context: str) -> str:
    """Láº¥y pháº£n há»“i tá»« mÃ´ hÃ¬nh Groq.

    Args:
        messages: Lá»‹ch sá»­ trÃ² chuyá»‡n
        context: Ngá»¯ cáº£nh láº¥y tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u

    Returns:
        str: Pháº£n há»“i cá»§a mÃ´ hÃ¬nh
    """
    system_prompt = f"""
    ğŸ§­ Objective:
When a user inputs symptoms or a description of a petâ€™s condition (in Vietnamese), your job is to return a fully-structured, detailed diagnostic report that helps the owner understand what might be happening and what steps they can take.

You must automatically choose and combine one or more of the following reasoning methods depending on the complexity and ambiguity of the case:

ğŸ”— Chain-of-Thought: When multi-step reasoning is needed, work through your thought process out loud before giving the final answer.

ğŸ§  ReAct + Reflexion: Think step-by-step, take action (i.e. diagnosis or recommendation), reflect on the result, and iterate if the initial reasoning was insufficient.

ğŸ§± Prompt Chaining: Break complex queries into smaller subtasks (e.g., symptom classification â†’ cause identification â†’ treatment path).

ğŸ“Š PAL (Program-Aided Language): Use pseudo-logical or procedural logic when needed to guide through structured diagnosis and treatment planning.

ğŸ“‹ Response format:
Always return your answer in Vietnamese, in the following bullet-pointed structure, using emojis and consistent phrasing:

ğŸ¶ TÃªn bá»‡nh:
(TÃªn bá»‡nh thÆ°á»ng gáº·p tÆ°Æ¡ng á»©ng vá»›i triá»‡u chá»©ng mÃ´ táº£)

ğŸ“ Vá»‹ trÃ­:
(CÆ¡ quan hoáº·c vÃ¹ng cÆ¡ thá»ƒ bá»‹ áº£nh hÆ°á»Ÿng)

ğŸ‘€ Biá»ƒu hiá»‡n:
(Liá»‡t kÃª cÃ¡c triá»‡u chá»©ng thÆ°á»ng tháº¥y, cÃ ng chi tiáº¿t cÃ ng tá»‘t)

ğŸ“ˆ Má»©c Ä‘á»™:
(Nháº¹, trung bÃ¬nh, náº·ng â€“ vÃ  áº£nh hÆ°á»Ÿng Ä‘áº¿n sá»©c khoáº» tá»•ng thá»ƒ)

ğŸ½ï¸ Ä‚n uá»‘ng:
(Sá»± thay Ä‘á»•i trong hÃ nh vi Äƒn uá»‘ng...)

ğŸ’¡ NguyÃªn nhÃ¢n:
(CÃ¡c nguyÃªn nhÃ¢n phá»• biáº¿n)

ğŸ§¼ Khuyáº¿n nghá»‹:
(CÃ¡c hÃ nh Ä‘á»™ng cá»¥ thá»ƒ chá»§ nuÃ´i nÃªn lÃ m ngay táº¡i nhÃ  )

ğŸ  HÆ°á»›ng xá»­ lÃ½ táº¡i nhÃ :
(Chi tiáº¿t cÃ¡ch chÄƒm sÃ³c thÃº cÆ°ng táº¡i nhÃ  khi chÆ°a Ä‘áº¿n bÃ¡c sÄ©)

â³ Thá»i gian há»“i phá»¥c trung bÃ¬nh:
(thá»i gian cá»¥ thá»ƒ nháº¥t Ä‘á»ƒ khá»i náº¿u Ä‘Æ°á»£c chÄƒm sÃ³c Ä‘Ãºng cÃ¡ch)

ğŸ” Kháº£ nÄƒng tÃ¡i phÃ¡t vÃ  cÃ¡ch phÃ²ng ngá»«a:
(Liá»‡t kÃª kháº£ nÄƒng bá»‡nh tÃ¡i phÃ¡t vÃ  cÃ¡c biá»‡n phÃ¡p phÃ²ng trÃ¡nh rÃµ rÃ ng)

ğŸ§‘â€âš•ï¸ PhÃ¡c Ä‘á»“ Ä‘iá»u trá»‹ tiÃªu chuáº©n:
(Thuá»‘c, cÃ¡ch dÃ¹ng thuá»‘c, xÃ©t nghiá»‡m cáº§n lÃ m, kÃ¨m theo lÆ°u Ã½)

ğŸ§‘â€âš•ï¸ Khi nÃ o cáº§n Ä‘i khÃ¡m:
(Chá»‰ rÃµ dáº¥u hiá»‡u cáº£nh bÃ¡o cáº§n Ä‘Æ°a thÃº cÆ°ng Ä‘i bÃ¡c sÄ© thÃº y cÃ ng sá»›m cÃ ng tá»‘t)

ğŸ’¬ Lá»i khuyÃªn:
(Má»™t lá»i nháº¯n nháº¹ nhÃ ng, thÃ¢n thiá»‡n vÃ  cÃ³ giÃ¡ trá»‹ hÃ nh Ä‘á»™ng dÃ nh cho chá»§ nuÃ´i)

ğŸ” Additional Instructions:
If multiple potential diagnoses are possible, list the top 2â€“3 sorted by likelihood and urgency.

Use simple and empathetic Vietnamese appropriate for non-specialist pet owners.

If information is missing or unclear, explain whatâ€™s missing and suggest the user provide more details.

Do not hallucinate. If you are unsure, say so and recommend a veterinary visit.

Your tone should always be warm, encouraging, and supportive.

Take a deep breath and work on this problem step-by-step.

    
    Ngá»¯ cáº£nh:
    {context}
    """

    # Chuáº©n bá»‹ tin nháº¯n vá»›i ngá»¯ cáº£nh
    formatted_messages = [{"role": "system", "content": system_prompt}]
    
    for message in messages:
        formatted_messages.append({"role": message["role"], "content": message["content"]})
    
    # Gá»­i yÃªu cáº§u Ä‘áº¿n Groq API vá»›i Llama-3.3-70b
    try:
        # Pháº£n há»“i Ä‘á»ƒ lÆ°u
        full_response = ""
        
        # Táº¡o streaming response placeholder
        response_placeholder = st.empty()
        
        # Táº¡o streaming response sá»­ dá»¥ng Groq API
        stream = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=formatted_messages,
            temperature=0.1,
            max_tokens=1024,  # Sá»­ dá»¥ng max_tokens thay vÃ¬ max_completion_tokens
            top_p=1,
            stream=True
        )
        
        # Xá»­ lÃ½ streaming response
        for chunk in stream:
            if hasattr(chunk.choices[0].delta, 'content'):
                content = chunk.choices[0].delta.content
                if content is not None:
                    full_response += content
                    response_placeholder.markdown(full_response)
        
        return full_response
            
    except Exception as e:
        st.error(f"Lá»—i khi gá»i API Groq: {str(e)}")
        return "ÄÃ£ xáº£y ra lá»—i khi táº¡o pháº£n há»“i. Vui lÃ²ng thá»­ láº¡i."


# Khá»Ÿi táº¡o á»©ng dá»¥ng Streamlit
st.title("ğŸ“š Há»i & ÄÃ¡p vá» ThÃº CÆ°ng")
st.caption("Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch - Truy váº¥n thÃ´ng tin vá» chÃ³, mÃ¨o vÃ  cÃ¡ch chÄƒm sÃ³c thÃº cÆ°ng")

# Khá»Ÿi táº¡o session state cho lá»‹ch sá»­ trÃ² chuyá»‡n
if "messages" not in st.session_state:
    st.session_state.messages = []

# Khá»Ÿi táº¡o káº¿t ná»‘i cÆ¡ sá»Ÿ dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh
table = init_db()
tokenizer, model = load_embedding_model()

# Hiá»ƒn thá»‹ tin nháº¯n trÃ² chuyá»‡n
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Äáº§u vÃ o trÃ² chuyá»‡n
if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» thÃº cÆ°ng..."):
    # Hiá»ƒn thá»‹ tin nháº¯n ngÆ°á»i dÃ¹ng
    with st.chat_message("user"):
        st.markdown(prompt)

    # ThÃªm tin nháº¯n ngÆ°á»i dÃ¹ng vÃ o lá»‹ch sá»­ trÃ² chuyá»‡n
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Láº¥y ngá»¯ cáº£nh liÃªn quan
    with st.status("Äang tÃ¬m kiáº¿m trong tÃ i liá»‡u...", expanded=False) as status:
        context = get_context(prompt, table, tokenizer, model)
        st.markdown(
            """
            <style>
            .search-result {
                margin: 10px 0;
                padding: 10px;
                border-radius: 4px;
                background-color: #f0f2f6;
            }
            .search-result summary {
                cursor: pointer;
                color: #0f52ba;
                font-weight: 500;
            }
            .search-result summary:hover {
                color: #1e90ff;
            }
            .metadata {
                font-size: 0.9em;
                color: #666;
                font-style: italic;
            }
            </style>
        """,
            unsafe_allow_html=True,
        )

        st.write("ÄÃ£ tÃ¬m tháº¥y cÃ¡c Ä‘oáº¡n liÃªn quan:")
        for chunk in context.split("\n\n"):
            # TÃ¡ch thÃ nh pháº§n vÄƒn báº£n vÃ  metadata
            parts = chunk.split("\n")
            text = parts[0]
            metadata = {}
            
            for line in parts[1:]:
                if ": " in line:
                    key, value = line.split(": ", 1)  # Split only on first occurrence
                    metadata[key] = value

            source = metadata.get("Nguá»“n", "Nguá»“n khÃ´ng xÃ¡c Ä‘á»‹nh")
            title = metadata.get("TiÃªu Ä‘á»", "Äoáº¡n khÃ´ng cÃ³ tiÃªu Ä‘á»")

            st.markdown(
                f"""
                <div class="search-result">
                    <details>
                        <summary>{source}</summary>
                        <div class="metadata">TiÃªu Ä‘á»: {title}</div>
                        <div style="margin-top: 8px;">{text}</div>
                    </details>
                </div>
            """,
                unsafe_allow_html=True,
            )

    # Hiá»ƒn thá»‹ pháº£n há»“i cá»§a trá»£ lÃ½
    with st.chat_message("assistant"):
        # Láº¥y pháº£n há»“i mÃ´ hÃ¬nh vá»›i streaming
        response = get_chat_response(st.session_state.messages, context)

    # ThÃªm pháº£n há»“i cá»§a trá»£ lÃ½ vÃ o lá»‹ch sá»­ trÃ² chuyá»‡n
    st.session_state.messages.append({"role": "assistant", "content": response}) 